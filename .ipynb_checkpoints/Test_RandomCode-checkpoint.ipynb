{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://sfbay.craigslist.org/search/eby/apa?hasPic=1&availabilityMode=0') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print(len(posts)) #to double check I got 120 (elements/page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-25771b1b4d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m                    + \"&availabilityMode=0\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m#throw warning for status codes that are not 200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#build out the loop\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(\"https://sfbay.craigslist.org/search/sfc/apa?nh=20&nh=24&nh=17&nh=19&nh=22&nh=23&nh=27?\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            #    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "            #    sqft = np.nan\n",
    "            #    sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
